# THE SCRAPER
# Here's a list of what we need to do, in the exact order:

# Create a CSV file, open it, make it writeable
# Create a CSV writer to write data
# Write the column headers to the file
# Tell your computer which site(s) to visit
# Get the webpage, download HTML
# Create soup to...
# Select data from the soup aka webpage
# Write data with the CSV writer to the CSV file
# Save file


# CREATE A CSV FILE TO SAVE DATA IN 
f = open('scrapedData.csv', 'w', encoding='utf8', newline='')
# CREATE A WRITER TO WRITE DATA WITH A NEWLY CREATED CSV-FILE 
writer = csv.writer(f, delimiter=',')
# USE THE WRITER TO WRITE THE FIRST ROW, THE COLUMN HEADERS, TO THE FILE 
writer.writerow(['plantNamedocketNumber', 'licenseNumber', 'reactorType', 
                 'location', 'OwnerOperator', 'NRCRegion'])
# SET PAGE YOU WANT TO VISIT
url = 'https://www.nrc.gov/reactors/operating/list-power-reactor-units.html'
# REQUEST HTML OF PAGE USING REQUEST LIBRARY
page = requests.get(url)
# CREATE SOUP - PARSE HTML OF WEBPAGE
soup = BeautifulSoup(page.content, 'html.parser')
# SELECT TABLE FROM SOUP
table = soup.find('table')
# FOR EVERY ROW IN TABLE...
for row in table.find_all('tr'):
        # ...SAVE THE DATA IN EACH CELL TO CELLS
        cells = row.find_all(['th', 'td'])
        # ...GET THE DATA FROM EACH OF THE 6  CELLS AND SAVE TO VARIABLES 
        plantNamedocketNumber = cells[0].text
        licenseNumber = cells[1].text
        reactorType = cells[2].text
        location = cells[3].text
        OwnerOperator = cells[4].text
        NRCRegion = cells[5].text
        # ...COLLECT ALL DATA IN ROW CALLED rowData
        rowData = [plantNamedocketNumber, licenseNumber, reactorType, location, OwnerOperator, NRCRegion]
        # ...WRITE THE DATA TO THE CSV FILE
        writer.writerow(rowData)
# JUST CLOSING THE FILE ONCE WILL DO
f.close()
